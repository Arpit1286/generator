package com.cloudwick.generator.logEvents

import java.io.File
import java.util.concurrent.atomic.AtomicLong

// class generated by avro-tools
import com.cloudwick.generator.avro.LogEventAvroObj
import com.cloudwick.generator.utils._
import org.slf4j.LoggerFactory

import scala.collection.mutable.ArrayBuffer

/**
 * Persists mocked events to specified file.
 * This class can persist events in both Text formats (csv, tsv) & also in Avro.
 * @author ashrith 
 */
class Writer(eventsStartRange: Int,
             eventsEndRange: Int,
             counter: AtomicLong,
             sizeCounter: AtomicLong,
             config: OptionsConfig) extends Runnable with LazyLogging {
  lazy val utils = new Utils
  lazy val ipGenerator = new IPGenerator(config.ipSessionCount, config.ipSessionLength)
  lazy val logEventGen = new LogGenerator(ipGenerator)

  lazy val sleepTime = if(config.eventsPerSec == 0) 0 else 1000/config.eventsPerSec

  def threadName = Thread.currentThread().getName

  def formatEventToString(logEvent: LogEvent) = {
    s"${logEvent.ip} - - [${logEvent.timestamp}]" + " \"GET " + logEvent.request + " HTTP/1.1\"" +
    s" ${logEvent.responseCode} ${logEvent.responseSize} " +
      "\"-\" \"" + logEvent.userAgent + "\"\n"
  }

  def avroEvent(event: LogEvent) = {
    LogEventAvroObj.newBuilder()
      .setOriginatingIp(event.ip)
      .setClientIdentity("-")
      .setUserID("-")
      .setTimeStamp(event.timestamp)
      .setRequestType("GET")
      .setRequestPage(event.request)
      .setHTTPProtocolVersion("HTTP/1.1")
      .setResponseCode(event.responseCode)
      .setResponseSize(event.responseSize)
      .setReferrer("-")
      .setUserAgent(event.userAgent)
      .build()
  }

  def run() = {
    val totalEvents = eventsEndRange - eventsStartRange + 1
    var batchCount: Int = 0
    val outputFile = new File(config.filePath, s"mock_apache_$threadName.data").toString
    var fileHandlerText: FileHandler = null
    var fileHandlerAvro: AvroFileHandler[LogEventAvroObj] = null
    var kafkaHandler: KafkaHandler = null
    var kafkaAvroHandler: KafkaAvroHandler[LogEventAvroObj] = null
    var kinesisHandler: KinesisHandler = null
    var eventsAvro = new ArrayBuffer[LogEventAvroObj](config.flushBatch)
    var eventsText  = new ArrayBuffer[String](config.flushBatch)
    val ipGenerator = new IPGenerator(config.ipSessionCount, config.ipSessionLength)
    val logEventGenerator = new LogGenerator(ipGenerator)
    var logEvent: LogEvent = null

    logger.debug("Initializing thread: {} ({} - {})",
      threadName,
      eventsStartRange.toString,
      eventsEndRange.toString)

    try {
      if (config.destination == "file") {
        if (config.outputFormat == "avro") {
          fileHandlerAvro = new AvroFileHandler[LogEventAvroObj](outputFile, config.fileRollSize)
          fileHandlerAvro.openFile()
        } else {
          fileHandlerText = new FileHandler(outputFile, config.fileRollSize)
          fileHandlerText.openFile()
        }
      }
      else if (config.destination == "kafka") {
        if (config.outputFormat == "avro") {
          kafkaAvroHandler = new KafkaAvroHandler(config.kafkaBrokerList, config.kafkaTopicName)
        } else {
          kafkaHandler = new KafkaHandler(config.kafkaBrokerList, config.kafkaTopicName)
        }
      } else {
        kinesisHandler = new KinesisHandler(
          Credentials(config.awsAccessKey, config.awsSecretKey, config.awsEndPoint)
        )
        kinesisHandler.createStream(config.kinesisStreamName, config.kinesisShardCount, 60, 60)
      }
      // Start generating
      (eventsStartRange to eventsEndRange).foreach { eventCount =>
        Thread.sleep(sleepTime)
        batchCount += 1
        logEvent = logEventGenerator.eventGenerate
        sizeCounter.getAndAdd(logEvent.toString.getBytes.length)
        if (config.outputFormat == "avro") {
          eventsAvro += avroEvent(logEvent)
        } else {
          eventsText += formatEventToString(logEvent)
        }
        counter.getAndIncrement
        if (batchCount == config.flushBatch || batchCount == totalEvents) {
          if (config.destination == "file") {
            if (config.outputFormat == "avro") {
              fileHandlerAvro.publishBuffered(eventsAvro)
              eventsAvro.clear()
            } else {
              fileHandlerText.publishBuffered(eventsText)
              eventsText.clear()
            }
          } else if (config.destination == "kafka") {
            if (config.outputFormat == "avro") {
              kafkaAvroHandler.publishBuffered(eventsAvro)
              eventsAvro.clear()
            } else {
              kafkaHandler.publishBuffered(eventsText)
              eventsText.clear()
            }
          } else {
            kinesisHandler.publishBuffered(eventsText)
            eventsText.clear()
          }
          batchCount = 0
        }
      }
      logger.debug(s"Events generated by $threadName is: $totalEvents from ($eventsStartRange) to ($eventsEndRange)")
    } catch {
      case e: Exception =>
        logger.error("Error:: {}", e)
    }
    finally {
      if (config.destination == "file") {
        if (config.outputFormat == "avro") {
          fileHandlerAvro.close()
        } else {
          fileHandlerText.close()
        }
      } else if (config.destination == "kafka") {
        if (config.outputFormat == "avro") {
          kafkaAvroHandler.close()
        } else {
          kafkaHandler.close()
        }
      }
    }
  }
}
